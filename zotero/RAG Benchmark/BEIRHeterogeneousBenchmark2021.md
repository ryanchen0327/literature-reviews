---
dg-publish: true
category: literaturenote
tags: 
citekey: thakurBEIRHeterogeneousBenchmark2021
status: unread
dateread:
---
# Notes


_No personal notes yet._


# Key questions



---
> [!Cite]
> Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., & Gurevych, I. (2021, August 29). _BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models_. Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). [https://openreview.net/forum?id=wCu6T5xFjeJ](https://openreview.net/forum?id=wCu6T5xFjeJ)

> [!Synth]
> **Contribution**::  
>   
> **Related**:: 

> [!Metadata]
> **Authors**::Thakur, Nandan, Reimers, Nils, Rücklé, Andreas, Srivastava, Abhishek, Gurevych, Iryna
> **Title**:: BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models  
> **Year**:: 2021  
> **Citekey**:: thakurBEIRHeterogeneousBenchmark2021> **Item Type**:: conferencePaper

> [!Link]
> [Full Text PDF](file:///Users/ryanchen/Zotero/storage/PZWWIVVP/Thakur%20et%20al.%20-%202021%20-%20BEIR%20A%20Heterogeneous%20Benchmark%20for%20Zero-shot%20Evaluation%20of%20Information%20Retrieval%20Models.pdf)

> [!Abstract]
> Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction, and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems and contributes to accelerating progress towards more robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.
---

# Annotations

### Imported: 2025-06-03 2:56 pm



